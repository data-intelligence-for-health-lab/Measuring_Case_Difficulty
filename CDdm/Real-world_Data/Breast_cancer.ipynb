{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"   \n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2953,
     "status": "ok",
     "timestamp": 1672964642432,
     "user": {
      "displayName": "Eulee Kwon",
      "userId": "02607663235126237687"
     },
     "user_tz": 420
    },
    "id": "C-kmluVDKKTR",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import itertools\n",
    "from hyperopt.pyll.base import scope \n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK, STATUS_FAIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Exploring Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CT           False\n",
       "UCSize       False\n",
       "UCShape      False\n",
       "MA           False\n",
       "SECSize      False\n",
       "BN            True\n",
       "BC           False\n",
       "NN           False\n",
       "Mitoses      False\n",
       "Diagnosis    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers = [\"ID\",\"CT\",\"UCSize\",\"UCShape\",\"MA\",\"SECSize\",\"BN\",\"BC\",\"NN\",\"Mitoses\",\"Diagnosis\"]\n",
    "data = pd.read_csv('breast-cancer-wisconsin.csv', na_values='?',    \n",
    "                   header=None, index_col=['ID'], names = headers) \n",
    "data.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1672964642433,
     "user": {
      "displayName": "Eulee Kwon",
      "userId": "02607663235126237687"
     },
     "user_tz": 420
    },
    "id": "Itv6DB739jH0",
    "outputId": "4a3c5828-7c67-4579-a90f-214a72c7c085",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     CT  UCSize  UCShape  MA  SECSize    BN  BC  NN  Mitoses\n",
      "0     5       1        1   1        2   1.0   3   1        1\n",
      "1     5       4        4   5        7  10.0   3   2        1\n",
      "2     3       1        1   1        2   2.0   3   1        1\n",
      "3     6       8        8   1        3   4.0   3   7        1\n",
      "4     4       1        1   3        2   1.0   3   1        1\n",
      "..   ..     ...      ...  ..      ...   ...  ..  ..      ...\n",
      "694   3       1        1   1        3   2.0   1   1        1\n",
      "695   2       1        1   1        2   1.0   1   1        1\n",
      "696   5      10       10   3        7   3.0   8  10        2\n",
      "697   4       8        6   4        3   4.0  10   6        1\n",
      "698   4       8        8   5        4   5.0  10   4        1\n",
      "\n",
      "[699 rows x 9 columns]\n",
      "0      2\n",
      "1      2\n",
      "2      2\n",
      "3      2\n",
      "4      2\n",
      "      ..\n",
      "694    2\n",
      "695    2\n",
      "696    4\n",
      "697    4\n",
      "698    4\n",
      "Name: Diagnosis, Length: 699, dtype: int64\n",
      "Diagnosis\n",
      "2    458\n",
      "4    241\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data1 = data.copy()\n",
    "# separate labels from features\n",
    "y = data1['Diagnosis'] # labels\n",
    "X = data1.drop(['Diagnosis'], axis = 1)\n",
    "print(X)\n",
    "print(y)\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "694    0\n",
      "695    0\n",
      "696    1\n",
      "697    1\n",
      "698    1\n",
      "Name: Diagnosis, Length: 699, dtype: int64\n",
      "Diagnosis\n",
      "0    458\n",
      "1    241\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "y.replace({2: 0, 4: 1}, inplace=True)\n",
    "print(y)\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "UCI_X = X\n",
    "UCI_y = y\n",
    "UCI_X = UCI_X.to_numpy()\n",
    "UCI_y = UCI_y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset Setting\n",
    "log_file_name = './Breast_cancer.log'\n",
    "file_name = './Breast_cancer.xlsx'\n",
    "max_eval_a = 5\n",
    "max_eval_b = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CT', 'UCSize', 'UCShape', 'MA', 'SECSize', 'BN', 'BC', 'NN', 'Mitoses']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = X.columns\n",
    "names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Split the data to five folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1672964642433,
     "user": {
      "displayName": "Eulee Kwon",
      "userId": "02607663235126237687"
     },
     "user_tz": 420
    },
    "id": "yOuMbfuAKbuf",
    "outputId": "5e1a79b3-8dab-44ec-aeb5-3b1b6c8b61a7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Split the data to five groups and save each index in fold_indices \n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state= 0)\n",
    "# Save the index of each fold in a list\n",
    "fold_index = []\n",
    "for train_index, test_index in kfold.split(UCI_X):\n",
    "    fold_index.append((test_index)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1672964642434,
     "user": {
      "displayName": "Eulee Kwon",
      "userId": "02607663235126237687"
     },
     "user_tz": 420
    },
    "id": "HzipQIwa0oXQ",
    "outputId": "86430a2d-df19-4b02-ee15-fc8064e25ae9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 2, 3, 4], [1, 2, 3, 4, 0], [2, 3, 4, 0, 1], [3, 4, 0, 1, 2], [4, 0, 1, 2, 3]]\n"
     ]
    }
   ],
   "source": [
    "# Order of using folds\n",
    "fold_order = []\n",
    "lst = [0,1,2,3,4]\n",
    "for order in range(len(lst)):\n",
    "    fold_order.append(lst[order:] + lst[:order])\n",
    "print(fold_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "numbers = [5, 10, 15, 20]\n",
    "one_combinations_with_order = list(itertools.product(numbers))\n",
    "two_combinations_with_order = list(itertools.product(numbers, repeat=2))\n",
    "three_combinations = list(itertools.product(numbers, repeat=3))\n",
    "\n",
    "layer_neuron_orders = []\n",
    "layer_neuron_orders = one_combinations_with_order + two_combinations_with_order+three_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 440,
     "status": "ok",
     "timestamp": 1672964642868,
     "user": {
      "displayName": "Eulee Kwon",
      "userId": "02607663235126237687"
     },
     "user_tz": 420
    },
    "id": "mbfQragA3dQG",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Neural network model of model A\n",
    "def create_model_A(params):\n",
    "    # Create model\n",
    "    model = tf.keras.models.Sequential()\n",
    "    # First hidden layer with input shape\n",
    "    model.add(tf.keras.layers.Dense(params['hidden_layer_sizes'][0], input_shape=(train_x_model_A.shape[1],), activation=params['activation']))   \n",
    "    for i in range(1,len(params['hidden_layer_sizes'])):\n",
    "        # from second hidden layer to number of hidden layers\n",
    "        model.add(tf.keras.layers.Dense(params['hidden_layer_sizes'][i], activation=params['activation']))\n",
    "        # Ouput layer \n",
    "    model.add(tf.keras.layers.Dense(1,activation='sigmoid')) \n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=params['learnRate']), metrics=['accuracy'])  \n",
    "   \n",
    "    es = EarlyStopping(monitor='val_loss',mode='min', verbose=1, patience=10)  \n",
    "    result = model.fit(train_x_model_A, train_y_model_A, verbose=0, validation_split=0.3, \n",
    "                       batch_size = params['batch_size'], epochs = 100, \n",
    "                       callbacks=[es])\n",
    "    \n",
    "    validation_loss = np.amin(result.history['val_loss'])\n",
    "    validation_acc = np.amin(result.history['val_accuracy']) \n",
    "    #print('Best validation_loss of epoch:', validation_loss,'Best validation_acc of epoch:', validation_acc)\n",
    "    \n",
    "    return {'loss': validation_loss,\n",
    "            'acc': validation_acc,\n",
    "            'status': STATUS_OK, \n",
    "            'model': model,\n",
    "            'params': params}  \n",
    "\n",
    "\n",
    "# Neural network model of model B\n",
    "def create_model_B(params):\n",
    "    # Create model\n",
    "    model = tf.keras.models.Sequential()\n",
    "    # First hidden layer with input shape\n",
    "    model.add(tf.keras.layers.Dense(params['hidden_layer_sizes'][0], input_shape=(model_B_train_x.shape[1],), activation=params['activation']))   \n",
    "    for i in range(1,len(params['hidden_layer_sizes'])):\n",
    "        # from second hidden layer to number of hidden layers\n",
    "        model.add(tf.keras.layers.Dense(params['hidden_layer_sizes'][i], activation=params['activation']))\n",
    "        # Ouput layer \n",
    "    model.add(tf.keras.layers.Dense(1,activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=params['learnRate']), metrics=['accuracy'])  \n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss',mode='min', verbose=1, patience=10)  \n",
    "    result = model.fit(model_B_train_x, model_B_train_y, verbose=0, validation_split=0.3, \n",
    "                       batch_size = params['batch_size'], epochs = 100, \n",
    "                       callbacks=[es])\n",
    "    \n",
    "    validation_loss = np.amin(result.history['val_loss'])\n",
    "    validation_acc = np.amin(result.history['val_accuracy']) \n",
    "    #print('Best validation_loss of epoch:', validation_loss,'Best validation_acc of epoch:', validation_acc)\n",
    "    \n",
    "    return {'loss': validation_loss,\n",
    "            'acc': validation_acc, \n",
    "            'status': STATUS_OK, \n",
    "            'model': model,\n",
    "            'params': params}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 407,
     "status": "ok",
     "timestamp": 1672967045925,
     "user": {
      "displayName": "Eulee Kwon",
      "userId": "02607663235126237687"
     },
     "user_tz": 420
    },
    "id": "4NIl6swo_CHL",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def approach_2(UCI_X,UCI_y,folds):\n",
    "    ## First and Second folds\n",
    "    # First fold\n",
    "    X_fold = UCI_X[fold_index[folds[0]]]\n",
    "    y_fold = UCI_y[fold_index[folds[0]]]\n",
    "    d = pd.DataFrame(X_fold,columns=names)\n",
    "    d['label'] = y_fold   \n",
    "    fold_data0 = d\n",
    "    \n",
    "    # Second fold\n",
    "    X_fold = UCI_X[fold_index[folds[1]]]\n",
    "    y_fold = UCI_y[fold_index[folds[1]]]\n",
    "    d = pd.DataFrame(X_fold,columns=names)\n",
    "    d['label'] = y_fold   \n",
    "    fold_data1 = d\n",
    "\n",
    "    # train_data_for_model_A is concat of fold 1, fold 2\n",
    "    train_data_for_model_A = pd.concat([fold_data0,fold_data1],ignore_index=True)\n",
    "    #print(train_data_for_model_A)\n",
    "    \n",
    "    #Check the label is balanced\n",
    "    print(\"train_data_for_model_A:\",train_data_for_model_A['label'].value_counts())\n",
    "    \n",
    "\n",
    "    global train_x_model_A, train_y_model_A    \n",
    "    # train_data_for_model_A to x,y  \n",
    "    train_x_model_A =  train_data_for_model_A.iloc[:,:-1]\n",
    "    train_y_model_A =  train_data_for_model_A.iloc[:, -1]\n",
    "    \n",
    "    #StandardScaler be generated by fold 1 and fold 2\n",
    "    scaler = StandardScaler()\n",
    "    train_x_model_A = train_x_model_A.fillna(train_x_model_A.mean())\n",
    "    train_x_model_A = scaler.fit_transform(train_x_model_A)\n",
    "\n",
    "    \n",
    "    # new search space\n",
    "    search_space = {'learnRate': hp.choice('learnRate',[0.01,0.03,0.1]),\n",
    "                    'batch_size': scope.int(hp.choice('batch_size',[32,64,128])),\n",
    "                    'activation':hp.choice('activation',['relu','tanh']),\n",
    "                    'hidden_layer_sizes':hp.choice('hidden_layer_sizes',layer_neuron_orders)}\n",
    "    \n",
    "    trials = Trials()   \n",
    "    best = fmin(fn=create_model_A,\n",
    "                space=search_space,\n",
    "                algo=tpe.suggest,\n",
    "                max_evals=max_eval_a,\n",
    "                trials=trials,\n",
    "                verbose=False)\n",
    "\n",
    "    best_model_A = trials.results[np.argmin([r['loss'] for r in trials.results])]['model']\n",
    "    best_params_A = trials.results[np.argmin([r['loss'] for r in trials.results])]['params']\n",
    "    best_acc_A =  trials.results[np.argmin([r['loss'] for r in trials.results])]['acc']\n",
    "    best_loss_A =  trials.results[np.argmin([r['loss'] for r in trials.results])]['loss']\n",
    "    print(\"best_acc_A:\",best_acc_A,\"best_loss_A:\",best_loss_A)\n",
    "    print(\"best_params_A:\",best_params_A)   \n",
    " \n",
    "    \n",
    "    ## Thire and Fourth folds\n",
    "    # Third fold\n",
    "    X_fold = UCI_X[fold_index[folds[2]]]\n",
    "    y_fold = UCI_y[fold_index[folds[2]]]\n",
    "    d = pd.DataFrame(X_fold,columns=names)\n",
    "    d['label'] = y_fold   \n",
    "    fold_data2 = d\n",
    "    \n",
    "    # Fourth fold\n",
    "    X_fold = UCI_X[fold_index[folds[3]]]\n",
    "    y_fold = UCI_y[fold_index[folds[3]]]\n",
    "    d = pd.DataFrame(X_fold,columns=names)\n",
    "    d['label'] = y_fold   \n",
    "    fold_data3 = d\n",
    "    \n",
    "    # test data for model A with fold 3, fold 4\n",
    "    test_data_for_model_A = pd.concat([fold_data2,fold_data3],ignore_index=True)\n",
    "    print(\"test_data_for_model_A:\",test_data_for_model_A['label'].value_counts())\n",
    "    \n",
    "    \n",
    "    # test_data_for_model_A to x and answer\n",
    "    model_A_X_test = test_data_for_model_A.iloc[:,:-1]\n",
    "    answer = test_data_for_model_A.iloc[:, -1]\n",
    "\n",
    "    #print(\"before:\",model_A_X_test)\n",
    "    #Generated StandardScaler used for fold 3 and fold 4\n",
    "    model_A_X_test = model_A_X_test.fillna(train_x_model_A.mean())\n",
    "    model_A_X_test = scaler.transform(model_A_X_test)\n",
    "    model_A_X_test = pd.DataFrame(model_A_X_test)\n",
    "    \n",
    "    # Traind model A make predictions\n",
    "    model_A_predictions = (best_model_A.predict(model_A_X_test) > 0.5).astype(int)\n",
    "    \n",
    "    # Compare between answer and predictions from model A for fold 3, 4\n",
    "    id_df = pd.DataFrame()\n",
    "    id_df[\"actual\"] = answer\n",
    "    id_df[\"predicted\"] = model_A_predictions\n",
    "    incorrect = id_df.loc[id_df.actual != id_df.predicted]\n",
    "    \n",
    "    incorrect_index = []\n",
    "    incorrect_index = incorrect.index\n",
    "    print(\"number of incorrect predictions:\",len(incorrect_index),'\\n',incorrect[\"actual\"].value_counts())\n",
    "    sum_number_incorrect.append(len(incorrect_index))\n",
    "\n",
    "    # Provide 0 to wrong correctness, 1 to right correctness\n",
    "    # Append correctness to the fold 3, 4 dataset and find the hyperparameter\n",
    "    wrong_number = incorrect_index\n",
    "    correctness = []\n",
    "    for i in range(len(model_A_X_test)):\n",
    "        if model_A_X_test.index[i] in wrong_number:\n",
    "            correctness.append(0)\n",
    "        elif model_A_X_test.index[i] not in wrong_number:\n",
    "            correctness.append(1)\n",
    "        else:\n",
    "            print(\"error\")\n",
    "    correctness = pd.DataFrame(correctness,columns =['correctness'])\n",
    "    print(correctness['correctness'].value_counts())\n",
    "    \n",
    "\n",
    "    # Get the raw test data from fold 3, fold 4 (Before StandardScaler)\n",
    "    test_data_for_model_A = pd.concat([fold_data2,fold_data3],ignore_index=True)\n",
    "    # Drop the y \n",
    "    model_A_X_test = test_data_for_model_A.iloc[:,:-1]\n",
    "    #Generate New StandardScaler using fold 3 and fold 4 (will be used for fold 5)\n",
    "    model_A_X_test = model_A_X_test.fillna(model_A_X_test.mean())\n",
    "    model_A_X_test = scaler.fit_transform(model_A_X_test)    \n",
    "    \n",
    "    global model_B_train_x, model_B_train_y    \n",
    "    # fold 3, 4 data with correctness\n",
    "    model_B_train_x =  model_A_X_test\n",
    "    model_B_train_y =  correctness    \n",
    "\n",
    "    trials = Trials()   \n",
    "    best = fmin(fn=create_model_B,\n",
    "                space=search_space,\n",
    "                algo=tpe.suggest,\n",
    "                max_evals=max_eval_b,\n",
    "                trials=trials,\n",
    "                verbose=False)\n",
    "    \n",
    "    best_model_B = trials.results[np.argmin([r['loss'] for r in trials.results])]['model']\n",
    "    best_params_B = trials.results[np.argmin([r['loss'] for r in trials.results])]['params']\n",
    "    best_acc_B =  trials.results[np.argmin([r['loss'] for r in trials.results])]['acc']\n",
    "    best_loss_B =  trials.results[np.argmin([r['loss'] for r in trials.results])]['loss']\n",
    "    print(\"best_acc_B:\",best_acc_B,\"best_loss_B:\",best_loss_B)\n",
    "    print(\"best_params_B:\",best_params_B)\n",
    "    \n",
    "    \n",
    "    # Predict fold 5 label correctness\n",
    "    X_fold = UCI_X[fold_index[folds[4]]]\n",
    "    y_fold = UCI_y[fold_index[folds[4]]]\n",
    "    d = pd.DataFrame(X_fold,columns=names)\n",
    "    d['label'] = y_fold   \n",
    "    difficulty_data_for_model_B = d\n",
    "    print(\"difficulty_data_for_model_B:\",difficulty_data_for_model_B['label'].value_counts())   \n",
    "    \n",
    "    \n",
    "    difficulty_x = difficulty_data_for_model_B.iloc[:,:-1]\n",
    "    \n",
    "    #Generated StandardScaler from fold 3 and fold 4 used for fold 5\n",
    "    difficulty_x = difficulty_x.fillna(model_A_X_test.mean())\n",
    "    difficulty_x = scaler.transform(difficulty_x)      \n",
    "    \n",
    "    \n",
    "    predicted_difficulty = 1 - best_model_B.predict(difficulty_x) #By doing 1- Difficult case is closer to 1, Easy case is closer to 0\n",
    "    predicted_difficulty = predicted_difficulty[:,0].tolist()\n",
    "    \n",
    "    return(difficulty_data_for_model_B, predicted_difficulty)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Run code for five groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 499982,
     "status": "ok",
     "timestamp": 1672969326996,
     "user": {
      "displayName": "Eulee Kwon",
      "userId": "02607663235126237687"
     },
     "user_tz": 420
    },
    "id": "cqtFjm_X3h6G",
    "outputId": "b78c4bcc-83ab-49cd-cecb-fa272d8dacb7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "fold: [0, 1, 2, 3, 4]\n",
      "train_data_for_model_A: label\n",
      "0    180\n",
      "1    100\n",
      "Name: count, dtype: int64\n",
      "Epoch 20: early stopping\n",
      "Epoch 17: early stopping\n",
      "Epoch 11: early stopping\n",
      "Epoch 16: early stopping\n",
      "Epoch 13: early stopping\n",
      "best_acc_A: 0.9285714030265808 best_loss_A: 0.13842952251434326\n",
      "best_params_A: {'activation': 'tanh', 'batch_size': 128, 'hidden_layer_sizes': (20, 15, 20), 'learnRate': 0.1}\n",
      "test_data_for_model_A: label\n",
      "0    183\n",
      "1     97\n",
      "Name: count, dtype: int64\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "number of incorrect predictions: 7 \n",
      " actual\n",
      "0    6\n",
      "1    1\n",
      "Name: count, dtype: int64\n",
      "correctness\n",
      "1    273\n",
      "0      7\n",
      "Name: count, dtype: int64\n",
      "Epoch 67: early stopping\n",
      "Epoch 17: early stopping\n",
      "Epoch 11: early stopping\n",
      "Epoch 27: early stopping\n",
      "Epoch 33: early stopping\n",
      "Epoch 21: early stopping\n",
      "Epoch 76: early stopping\n",
      "Epoch 12: early stopping\n",
      "Epoch 19: early stopping\n",
      "Epoch 30: early stopping\n",
      "best_acc_B: 0.9523809552192688 best_loss_B: 0.0001594735513208434\n",
      "best_params_B: {'activation': 'tanh', 'batch_size': 32, 'hidden_layer_sizes': (5, 10), 'learnRate': 0.1}\n",
      "difficulty_data_for_model_B: label\n",
      "0    95\n",
      "1    44\n",
      "Name: count, dtype: int64\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "time per fold: 0:01:30.220288\n",
      "\n",
      "\n",
      "\n",
      "fold: [1, 2, 3, 4, 0]\n",
      "train_data_for_model_A: label\n",
      "0    180\n",
      "1    100\n",
      "Name: count, dtype: int64\n",
      "Epoch 13: early stopping\n",
      "Epoch 20: early stopping\n",
      "Epoch 21: early stopping\n",
      "Epoch 37: early stopping\n",
      "Epoch 41: early stopping\n",
      "best_acc_A: 0.9404761791229248 best_loss_A: 0.019661888480186462\n",
      "best_params_A: {'activation': 'tanh', 'batch_size': 128, 'hidden_layer_sizes': (15, 15, 15), 'learnRate': 0.1}\n",
      "test_data_for_model_A: label\n",
      "0    193\n",
      "1     86\n",
      "Name: count, dtype: int64\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "number of incorrect predictions: 16 \n",
      " actual\n",
      "1    13\n",
      "0     3\n",
      "Name: count, dtype: int64\n",
      "correctness\n",
      "1    263\n",
      "0     16\n",
      "Name: count, dtype: int64\n",
      "Epoch 20: early stopping\n",
      "Epoch 32: early stopping\n",
      "Epoch 17: early stopping\n",
      "Epoch 23: early stopping\n",
      "Epoch 18: early stopping\n",
      "Epoch 47: early stopping\n",
      "Epoch 20: early stopping\n",
      "Epoch 16: early stopping\n",
      "Epoch 20: early stopping\n",
      "Epoch 12: early stopping\n",
      "best_acc_B: 0.9523809552192688 best_loss_B: 0.10988671332597733\n",
      "best_params_B: {'activation': 'relu', 'batch_size': 64, 'hidden_layer_sizes': (10, 10, 5), 'learnRate': 0.01}\n",
      "difficulty_data_for_model_B: label\n",
      "0    85\n",
      "1    55\n",
      "Name: count, dtype: int64\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "time per fold: 0:01:29.278219\n",
      "\n",
      "\n",
      "\n",
      "fold: [2, 3, 4, 0, 1]\n",
      "train_data_for_model_A: label\n",
      "0    183\n",
      "1     97\n",
      "Name: count, dtype: int64\n",
      "Epoch 26: early stopping\n",
      "Epoch 12: early stopping\n",
      "Epoch 11: early stopping\n",
      "Epoch 15: early stopping\n",
      "Epoch 12: early stopping\n",
      "best_acc_A: 0.9404761791229248 best_loss_A: 2.089960253215395e-05\n",
      "best_params_A: {'activation': 'relu', 'batch_size': 64, 'hidden_layer_sizes': (20, 15, 10), 'learnRate': 0.1}\n",
      "test_data_for_model_A: label\n",
      "0    180\n",
      "1     99\n",
      "Name: count, dtype: int64\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "number of incorrect predictions: 8 \n",
      " actual\n",
      "0    6\n",
      "1    2\n",
      "Name: count, dtype: int64\n",
      "correctness\n",
      "1    271\n",
      "0      8\n",
      "Name: count, dtype: int64\n",
      "Epoch 48: early stopping\n",
      "Epoch 13: early stopping\n",
      "Epoch 32: early stopping\n",
      "Epoch 60: early stopping\n",
      "Epoch 35: early stopping\n",
      "Epoch 20: early stopping\n",
      "Epoch 23: early stopping\n",
      "Epoch 12: early stopping\n",
      "Epoch 34: early stopping\n",
      "Epoch 33: early stopping\n",
      "best_acc_B: 0.976190447807312 best_loss_B: 0.005763407330960035\n",
      "best_params_B: {'activation': 'relu', 'batch_size': 32, 'hidden_layer_sizes': (20, 20, 10), 'learnRate': 0.03}\n",
      "difficulty_data_for_model_B: label\n",
      "0    95\n",
      "1    45\n",
      "Name: count, dtype: int64\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "time per fold: 0:01:29.318365\n",
      "\n",
      "\n",
      "\n",
      "fold: [3, 4, 0, 1, 2]\n",
      "train_data_for_model_A: label\n",
      "0    193\n",
      "1     86\n",
      "Name: count, dtype: int64\n",
      "Epoch 13: early stopping\n",
      "Epoch 11: early stopping\n",
      "Epoch 14: early stopping\n",
      "Epoch 13: early stopping\n",
      "Epoch 13: early stopping\n",
      "best_acc_A: 0.9642857313156128 best_loss_A: 0.07357370108366013\n",
      "best_params_A: {'activation': 'relu', 'batch_size': 64, 'hidden_layer_sizes': (20, 20, 5), 'learnRate': 0.03}\n",
      "test_data_for_model_A: label\n",
      "0    180\n",
      "1    100\n",
      "Name: count, dtype: int64\n",
      "9/9 [==============================] - 0s 994us/step\n",
      "number of incorrect predictions: 15 \n",
      " actual\n",
      "0    9\n",
      "1    6\n",
      "Name: count, dtype: int64\n",
      "correctness\n",
      "1    265\n",
      "0     15\n",
      "Name: count, dtype: int64\n",
      "Epoch 24: early stopping\n",
      "Epoch 12: early stopping\n",
      "Epoch 19: early stopping\n",
      "Epoch 33: early stopping\n",
      "Epoch 14: early stopping\n",
      "Epoch 28: early stopping\n",
      "Epoch 14: early stopping\n",
      "Epoch 13: early stopping\n",
      "Epoch 24: early stopping\n",
      "Epoch 19: early stopping\n",
      "best_acc_B: 0.9047619104385376 best_loss_B: 0.14689502120018005\n",
      "best_params_B: {'activation': 'relu', 'batch_size': 32, 'hidden_layer_sizes': (10, 15, 10), 'learnRate': 0.1}\n",
      "difficulty_data_for_model_B: label\n",
      "0    85\n",
      "1    55\n",
      "Name: count, dtype: int64\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "time per fold: 0:01:15.788474\n",
      "\n",
      "\n",
      "\n",
      "fold: [4, 0, 1, 2, 3]\n",
      "train_data_for_model_A: label\n",
      "0    180\n",
      "1     99\n",
      "Name: count, dtype: int64\n",
      "Epoch 30: early stopping\n",
      "Epoch 18: early stopping\n",
      "Epoch 15: early stopping\n",
      "Epoch 11: early stopping\n",
      "Epoch 13: early stopping\n",
      "best_acc_A: 0.988095223903656 best_loss_A: 0.014754627831280231\n",
      "best_params_A: {'activation': 'tanh', 'batch_size': 32, 'hidden_layer_sizes': (10, 15, 15), 'learnRate': 0.1}\n",
      "test_data_for_model_A: label\n",
      "0    180\n",
      "1    100\n",
      "Name: count, dtype: int64\n",
      "9/9 [==============================] - 0s 10ms/step\n",
      "number of incorrect predictions: 12 \n",
      " actual\n",
      "0    10\n",
      "1     2\n",
      "Name: count, dtype: int64\n",
      "correctness\n",
      "1    268\n",
      "0     12\n",
      "Name: count, dtype: int64\n",
      "Epoch 14: early stopping\n",
      "Epoch 12: early stopping\n",
      "Epoch 19: early stopping\n",
      "Epoch 24: early stopping\n",
      "Epoch 15: early stopping\n",
      "Epoch 19: early stopping\n",
      "Epoch 24: early stopping\n",
      "Epoch 17: early stopping\n",
      "Epoch 17: early stopping\n",
      "Epoch 22: early stopping\n",
      "best_acc_B: 0.976190447807312 best_loss_B: 0.0497322753071785\n",
      "best_params_B: {'activation': 'relu', 'batch_size': 128, 'hidden_layer_sizes': (10, 5, 10), 'learnRate': 0.03}\n",
      "difficulty_data_for_model_B: label\n",
      "0    98\n",
      "1    42\n",
      "Name: count, dtype: int64\n",
      "5/5 [==============================] - 0s 872us/step\n",
      "time per fold: 0:01:15.394626\n",
      "Overall time per fold: 0:07:00.000697\n"
     ]
    }
   ],
   "source": [
    "Final_data_save = []\n",
    "Final_difficulty_save = []\n",
    "sum_number_incorrect = []\n",
    "\n",
    "overall_start_time = datetime.now()\n",
    "for i in range(len(fold_order)):\n",
    "    start_time = datetime.now()\n",
    "    folds = fold_order[i]\n",
    "    print('\\n\\n\\nfold:',folds)\n",
    "    difficulty_data_for_model_B,difficulty = approach_2(UCI_X,UCI_y,folds)\n",
    "    Final_data_save.append(difficulty_data_for_model_B)\n",
    "    Final_difficulty_save.append(difficulty)\n",
    "    end_time = datetime.now()\n",
    "    fold_calculation_time = end_time - start_time\n",
    "    print(\"time per fold:\",fold_calculation_time)\n",
    "\n",
    "overall_end_time = datetime.now()\n",
    "overall_calculation_time = overall_end_time - overall_start_time\n",
    "print(\"Overall time per fold:\",overall_calculation_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 16, 8, 15, 12]\n",
      "58\n"
     ]
    }
   ],
   "source": [
    "print(sum_number_incorrect)\n",
    "print(sum(sum_number_incorrect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 16, 8, 15, 12]\n",
      "58\n"
     ]
    }
   ],
   "source": [
    "dataframe_temp = Final_data_save\n",
    "# Merge all the difficulties from the folds\n",
    "for i in range(len(fold_order)):\n",
    "    dataframe_temp[i]['difficulty'] = Final_difficulty_save[i]\n",
    "    \n",
    "new_column_names = X.columns.values.tolist()\n",
    "new_column_names.extend(['label','difficulty'])\n",
    "\n",
    "temp_df= pd.DataFrame(np.row_stack([dataframe_temp[0], dataframe_temp[1], dataframe_temp[2], dataframe_temp[3], dataframe_temp[4]]), \n",
    "                               columns=new_column_names)\n",
    "\n",
    "writer = pd.ExcelWriter(file_name)\n",
    "temp_df.to_excel(writer,header=True,index=False)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7iAf0H-0lgDC",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNOsWRLdil4XTkkH37zOUrv",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}